// services/aiService.js
// LangChain AI service for chatbot functionality
// Handles model initialization, memory management, and chat responses

import { ChatOpenAI } from "@langchain/openai";
import { ChatGroq } from "@langchain/groq";
import { BufferMemory } from "langchain/memory";
import { ConversationChain } from "langchain/chains";
import { PromptTemplate } from "@langchain/core/prompts";
import dotenv from 'dotenv';

dotenv.config();

// Model configurations
const MODEL_CONFIGS = {
  "gpt-3.5-turbo": {
    provider: "openai",
    modelName: "gpt-3.5-turbo",
    temperature: 0.7,
    maxTokens: 1000
  },
  "gpt-4": {
    provider: "openai",
    modelName: "gpt-4",
    temperature: 0.7,
    maxTokens: 1500
  },
  "llama-3.1-70b": {
    provider: "groq",
    modelName: "llama-3.1-70b-versatile",
    temperature: 0.7,
    maxTokens: 2000
  },
  "llama-3.1-8b": {
    provider: "groq", 
    modelName: "llama-3.1-8b-instant",
    temperature: 0.7,
    maxTokens: 2000
  }
};

// Language-specific system prompts with cultural awareness
const SYSTEM_PROMPTS = {
  en: {
    template: `You are a helpful AI assistant. You provide clear, accurate, and helpful responses to user questions. 
Be friendly, professional, and engaging in your responses. If you don't know something, admit it honestly.

Current conversation:
{history}

Human: {input}
Assistant:`
  },
  ar: {
    template: `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…Ø­ØªØ±Ù… ÙˆÙ…Ù‡Ø°Ø¨. ØªØªØ­Ø¯Ø« Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø¨ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ ÙˆØ§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø©.

Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ù…Ù‡Ù…Ø© Ù„Ù„Ø±Ø¯:
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ù…Ø¹Ø§ØµØ±Ø© (ØªØ¬Ù†Ø¨ Ø§Ù„Ø¹Ø§Ù…ÙŠØ© ØªÙ…Ø§Ù…Ø§Ù‹)
- Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø§Ø¨Ø¹ Ø§Ù„Ø±Ø³Ù…ÙŠ ÙˆØ§Ù„Ù…Ù‡Ù†ÙŠ Ù…Ø¹ Ø§Ù„ÙˆØ¯ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
- Ø§Ø¨Ø¯Ø£ Ø¨Ø§Ù„ØªØ­ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© (Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…ØŒ Ù…Ø±Ø­Ø¨Ø§Ù‹ØŒ Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹)
- Ø§Ø³ØªØ®Ø¯Ù… Ø£Ù„ÙØ§Ø¸ Ø§Ù„ØªÙ‚Ø¯ÙŠØ± ÙˆØ§Ù„Ø§Ø­ØªØ±Ø§Ù… (Ø­Ø¶Ø±ØªÙƒØŒ ØªÙƒØ±Ù…Ø§Ù‹ØŒ Ù…Ù† ÙØ¶Ù„ÙƒØŒ Ø¥Ø°Ø§ Ø³Ù…Ø­Øª)
- Ø±Ø§Ø¹Ù Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© ÙˆØ§Ù„Ø¯ÙŠÙ†ÙŠØ© (ØªØ¬Ù†Ø¨ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø­Ø³Ø§Ø³Ø©)
- ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆÙ…Ø³Ø§Ø¹Ø¯Ø§Ù‹ Ù…Ø¹ Ø§Ù„ØªÙˆØ§Ø¶Ø¹ Ø§Ù„Ù„Ø§Ø¦Ù‚
- Ø§Ø®ØªØªÙ… Ø¨Ø¯Ø¹ÙˆØ© Ù…Ù‡Ø°Ø¨Ø© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£ØµÙŠÙ„Ø© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ù…Ø¹Ø±Ù‘Ø¨Ø© Ø¹Ù†Ø¯Ù…Ø§ Ø£Ù…ÙƒÙ†

Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:
{history}

Ø§Ù„Ø¥Ù†Ø³Ø§Ù†: {input}
Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:`
  }
};

// Memory storage for each user
const userMemories = new Map();
const userChains = new Map();

/**
 * Initialize a chat model based on the model name
 * @param {string} modelName - The name of the model to initialize
 * @returns {ChatOpenAI|ChatGroq} The initialized chat model
 */
export const initializeChatModel = (modelName) => {
  try {
    const config = MODEL_CONFIGS[modelName];
    if (!config) {
      throw new Error(`Unsupported model: ${modelName}`);
    }

    let chatModel;

    if (config.provider === "openai") {
      chatModel = new ChatOpenAI({
        modelName: config.modelName,
        temperature: config.temperature,
        maxTokens: config.maxTokens,
        openAIApiKey: process.env.OPENAI_API_KEY
      });
    } else if (config.provider === "groq") {
      chatModel = new ChatGroq({
        model: config.modelName,
        temperature: config.temperature,
        maxTokens: config.maxTokens,
        apiKey: process.env.GROQ_API_KEY
      });
    } else {
      throw new Error(`Unsupported provider: ${config.provider}`);
    }

    console.log(`âœ… Initialized ${modelName} chat model`);
    return chatModel;
  } catch (error) {
    console.error(`âŒ Error initializing chat model ${modelName}:`, error);
    throw error;
  }
};

/**
 * Get or create conversation memory for a user
 * @param {number} userId - The user ID
 * @param {string} conversationId - The conversation ID (optional)
 * @returns {ConversationBufferMemory} The conversation memory
 */
const getUserMemory = (userId, conversationId = 'default') => {
  const memoryKey = `${userId}-${conversationId}`;
  
  if (!userMemories.has(memoryKey)) {
    const memory = new BufferMemory({
      memoryKey: "history",
      inputKey: "input",
      outputKey: "response",
      returnMessages: false
    });
    userMemories.set(memoryKey, memory);
    console.log(`ðŸ“ Created new memory for user ${userId}, conversation ${conversationId}`);
  }
  
  return userMemories.get(memoryKey);
};

/**
 * Get or create conversation chain for a user
 * @param {number} userId - The user ID
 * @param {string} modelName - The model name
 * @param {string} language - The language preference
 * @param {string} conversationId - The conversation ID
 * @returns {ConversationChain} The conversation chain
 */
const getUserChain = (userId, modelName, language, conversationId = 'default') => {
  const chainKey = `${userId}-${conversationId}-${modelName}-${language}`;
  
  if (!userChains.has(chainKey)) {
    const chatModel = initializeChatModel(modelName);
    const memory = getUserMemory(userId, conversationId);
    const promptTemplate = SYSTEM_PROMPTS[language] || SYSTEM_PROMPTS.en;
    
    const prompt = PromptTemplate.fromTemplate(promptTemplate.template);
    
    const chain = new ConversationChain({
      llm: chatModel,
      memory: memory,
      prompt: prompt,
      verbose: process.env.NODE_ENV === 'development'
    });
    
    userChains.set(chainKey, chain);
    console.log(`ðŸ”— Created new conversation chain for user ${userId}`);
  }
  
  return userChains.get(chainKey);
};

/**
 * Get chat response for a user message
 * @param {number} userId - The user ID
 * @param {string} message - The user message
 * @param {string} modelName - The model name to use
 * @param {string} language - The language preference (en/ar)
 * @param {string} conversationId - The conversation ID (optional)
 * @returns {Promise<string>} The AI response
 */
export const getChatResponse = async (userId, message, modelName = "gpt-3.5-turbo", language = "en", conversationId = "default") => {
  try {
    // Validate inputs
    if (!userId || !message) {
      throw new Error('User ID and message are required');
    }

    if (!MODEL_CONFIGS[modelName]) {
      throw new Error(`Unsupported model: ${modelName}`);
    }

    if (!SYSTEM_PROMPTS[language]) {
      console.warn(`Unsupported language: ${language}, defaulting to English`);
      language = "en";
    }

    console.log(`ðŸ’¬ Processing message for user ${userId} with ${modelName} in ${language}`);

    // Get or create conversation chain
    const chain = getUserChain(userId, modelName, language, conversationId);

    // Get response from the chain
    const response = await chain.call({
      input: message.trim()
    });

    const aiResponse = response.response || response.text || "I apologize, but I couldn't generate a response.";

    console.log(`âœ… Generated response for user ${userId}: ${aiResponse.substring(0, 100)}...`);
    return aiResponse;

  } catch (error) {
    console.error(`âŒ Error getting chat response:`, error);
    
    // Return appropriate error message based on language
    const errorMessage = language === 'ar' 
      ? 'Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„ØªÙƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.'
      : 'I apologize, but I encountered an error while processing your message. Please try again.';
    
    throw new Error(errorMessage);
  }
};

/**
 * Clear conversation memory for a user
 * @param {number} userId - The user ID
 * @param {string} conversationId - The conversation ID (optional)
 */
export const clearUserMemory = (userId, conversationId = 'default') => {
  const memoryKey = `${userId}-${conversationId}`;
  const chainKeys = Array.from(userChains.keys()).filter(key => key.startsWith(memoryKey));
  
  // Remove memory
  if (userMemories.has(memoryKey)) {
    userMemories.delete(memoryKey);
    console.log(`ðŸ—‘ï¸ Cleared memory for user ${userId}, conversation ${conversationId}`);
  }
  
  // Remove associated chains
  chainKeys.forEach(key => {
    userChains.delete(key);
    console.log(`ðŸ—‘ï¸ Cleared chain: ${key}`);
  });
};

/**
 * Get memory statistics
 * @returns {Object} Memory usage statistics
 */
export const getMemoryStats = () => {
  return {
    totalMemories: userMemories.size,
    totalChains: userChains.size,
    memoryKeys: Array.from(userMemories.keys()),
    chainKeys: Array.from(userChains.keys())
  };
};

/**
 * Validate API keys
 * @returns {Object} API key validation status
 */
export const validateApiKeys = () => {
  const openaiKey = process.env.OPENAI_API_KEY;
  const groqKey = process.env.GROQ_API_KEY;
  
  return {
    openai: !!openaiKey && openaiKey.startsWith('sk-'),
    groq: !!groqKey && groqKey.length > 20,
    hasAnyKey: !!(openaiKey || groqKey)
  };
};

// Export all functions
export default {
  initializeChatModel,
  getChatResponse,
  clearUserMemory,
  getMemoryStats,
  validateApiKeys
};